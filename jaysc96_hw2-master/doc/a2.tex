\documentclass{article}

\usepackage[]{algorithm2e}

\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{listings} % For displaying code

\begin{document}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}
\newcommand{\argmin}[1]{\mathop{\hbox{argmin}}_{#1}}
\newcommand{\argmax}[1]{\mathop{\hbox{argmax}}_{#1}}
\def\R{\mathbb{R}}
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}
\def\answer#1{\iftoggle{answers}{\blu{Answer}:\\#1}}
\def\rubric#1{Rubric: \{#1\}}{}
\newcommand{\code}[1]{\lstinputlisting[language=Matlab]{#1}}


\title{CPSC 340 Assignment 2 (due Sunday, February 5 at 11:59pm)}
\author{K-Nearest Neighbours, Random Forests, K-Means, Density-Based Clustering}
\date{}
\maketitle

\section*{Instructions}
\rubric{mechanics:3}

\textbf{IMPORTANT!!!!! Before proceeding, please carefully read the general homework instructions at} \url{https://github.ubc.ca/cpsc340/home/blob/master/homework_instructions.md}.
You need to be signed in to github.ubc.ca in order to view this file. If you can't sign in, email Mike.

Other notes:
\begin{itemize}
\item We use \blu{blue} to highlight the deliverables that you must answer/do/submit with the assignment.
\item As requested, the number of points per questions are given below. if you see something like ``\rubric{code:3, reasoning:3}'' you can interpret it as ``this part is worth 6 points: 3 for code and 3 for the written portion.'' The points for ``mechanics'' above are for following the homework instructions. 
\item Some of the provided code uses \texttt{plt.show} to render images to your screen. In some circumstances, matplotlib behaves weirdly and creates the figure behind other windows in a way that makes it hard to see. It then looks like the code is running indefinitely but actually it's just waiting for you to close the (elusive) figure. If this happens and it annoys you, you can change the calls to \texttt{plt.show} to \texttt{plt.savefig}, which just saves the image to a file instead of having it pop up on your screen.
\item You may want to add sections to \emph{main.py}. To do this, add an \texttt{elif} statement in the same format as the existing ones, but also make sure to add the new option to the set of \texttt{choices} at the top of the file. If you try to run an option that's not listed in \texttt{choices} you will get an error.
\item If you're using Python 2.7, you'll need to grab the alternate data at \url{https://github.ubc.ca/cpsc340/home/tree/master/assignments/hw2/data_python2.zip?raw=true}, which is Pickled using an older Pickle protocol. You don't have to overwrite the old data; once you copy in the Python 2 data into a directory called \emph{data\_python2}, you can tell the code to use it by changing the \texttt{DATA\string_DIR} variable at the top of \emph{utils.py}.
\end{itemize}

\section{K-Nearest Neighbours}

In this question we revisit the \emph{citiesSmall} dataset from the previous assignment.\footnote{For those who had trouble unpickling the data, you can use the alternate version that you used for hw1.}  In this dataset, nearby points tend to receive the same class label because they are part of the same state. This indicates that a $k$-nearest neighbours classifier might be a better choice than a decision tree. The file \emph{knn.py} has implemented the training function for a $k$-nearest neighbour classifier (which is to just memorize the data).


\subsection{KNN Prediction}
\rubric{code:3, reasoning:3}

Fill in the \emph{predict} function in \emph{knn.py} so that the model file implements the $k$-nearest neighbour prediction rule. You should Euclidean distance, and may numpy's \emph{sort} and/or \emph{argsort} functions useful. You can also use \emph{utils.euclidean\_dist\_squared}, which computes the squared Euclidean distances between all pairs of points in two matrices.
\blu{
\enum{
\item Hand in your predict function.
\item Report  the training and test error obtained on the \emph{citiesSmall} dataset for $k=1$, $k=3$, and $k=10$.
\item Hand in the plot generated by \emph{utils.plot\_2dclassifier2Dplot} on the \emph{citiesSmall} dataset for $k=1$. 
\item Why is the training error $0$ for $k=1$?
\item If you didn't have an explicit test set, how would you choose $k$?
}}

\enum{
\item \blu{\href{https://github.ubc.ca/cpsc340/jaysc96_hw2/blob/master/code/knn.py}{knn.py}}
\item Training error for $k=1$ is $0$, Testing error for $k=1$ is 0.0645
\newline Training error for $k=3$ is $0.06$, Testing error for $k=3$ is $0.092$
\newline Training error for $k=10$ is $0.2$, Testing error for $k=10$ is $0.23$
\item \begin{center}
\fig{0.7}{../figs/2_1aCitiesSmall} %
\end{center}
\item When $k=1$ the training error is 0 as the closest cities tend to be in the same state and hence have the same classifier.
\item If we didn’t have an explicit test set, we could choose $k$ by looking for the smallest distance between points with different classifiers in the given data, and then evaluate its $position$ including points with the same classifier and then using $(position-1)$ as $k$.
}

\subsection{Condensed Nearest Neighbours}
\rubric{code:3, reasoning:5}

The dataset \emph{citiesBig1} contains a version of this dataset with more than 30 times as many cities. KNN can obtain a lower test error if it's trained on this dataset, but the prediction time will be very slow. A common strategy for applying KNN to huge datasets is called \emph{condensed nearest neighbours}, and the main idea is to only store a \emph{subset} of the training examples (and to only compare to these examples when making predictions). If the subset is small, then prediction will be faster.

The most common variation of this algorithm works as follows:

\begin{algorithm}[H]
 initialize subset with first training example\;
 \For{each subsequent training example}{
  \eIf{the example is incorrectly classified by the KNN classifier using the current subset}{
   add the current example to the subset\;
   }{
   do \emph{not} add the current example to the subset (do nothing)\;
  }
 }
 \caption{Condensed Nearest Neighbours}
\end{algorithm}
Implement the \emph{condensed nearest neighbours} algorithm as described above in a file called \emph{cnn.py} with \emph{fit} and \emph{predict} functions. Note: if the size of the subset is less than $k$, you can take all examples in the subset as the current ``neighbours".
\blu{
\enum{
\item Hand in your \emph{cnn.py} code.
\item Report the training and testing errors, as well as the number of variables in the subset, on the\\\emph{citiesBig1} dataset with $k=1$.
\item Hand in the plot generated by \emph{utils.plot\_2dclassifier} on the \emph{citiesBig1} dataset for $k=1$.
\item Why is the training error with $k=1$ now greater than $0$?
\item If you have $s$ examples in the subset, what is the cost of running the predict function on $t$ test examples in terms of $n$, $d$, $t$, and $s$?
\item Try out your function on the dataset \emph{citiesBig2}. Why are the  test error \emph{and} training error so high (even for $k=1$) for this method on this dataset?
}
}
\enum{
\item \blu{\href{https://github.ubc.ca/cpsc340/jaysc96_hw2/blob/master/code/cnn.py}{cnn.py}}
\item Training error $=0.00753$, Testing error $=0.01757$
\newline Number of variables $=457$
\item \begin{center}
\fig{0.7}{../figs/2_1bCitiesBig} %
\end{center}
\item The training error is now greater than 0 since the final model dataset doesn’t include all the points and hence they are further spaced apart, so the closest city when predicting is not necessarily in the same state and may have a different classifier.
\item $cost = O(st)$
\item For citiesBig2 the training and testing errors are higher since predictions made with the early subset were probably the same by chance and hence valuable data points were not 
}

\section{Random Forests}

The dataset \emph{vowels} contains a supervised learning dataset where we are trying to predict which of the 11 ``steady-state'' English vowels that a speaker is trying to pronounce.

\subsection{Random Trees}
\rubric{code:4, reasoning:2}

The functions \emph{decision\_tree.py} and \emph{decision\_stump.py} implement a (non-random) decision tree that is built using greedy recursive splitting and information gain. The variant \emph{random\_tree.py} calls a \emph{random\_stump.py} (which is not included) to fit a random decision tree.
\blu{
\enum{
\item Make a plot of the training error and the test error for the \emph{decision\_tree} model, as the \emph{max\_depth} parameter is varied from 1 to 15.
\item Why does the \emph{decision\_tree} function terminate if you set the \emph{max\_depth} parameter to $\infty$?
\item Copy the \emph{decision\_stump.py} file to a new file called \emph{random\_stump.py}. Modify the \emph{random\_stump.py} so that it only considers $\lfloor \sqrt{d} \rfloor$ randomly-chosen features.\footnote{The notation $\lfloor x\rfloor$ means the ``floor'' of $x$, or ``$x$ rounded down''. You can compute this with \texttt{np.floor(x)} or \texttt{math.floor(x)}.} Hand in the new training function and make a plot of the training and test error of the now-working \emph{random\_tree} model as max\_depth is varied from 1 to 15 with this method (it should not be the same every time, so just include one run of the method).
\item Make a third training/test plot where you use the \emph{decision\_tree} model but for each max\_depth you train on a different bootstrap sample of the training data (but evaluate the training error on the original training data).
}
}

\enum{
\item \begin{center}
\fig{0.7}{../figs/2_2a1DecisionTree} %
\end{center}
\item Although the depth never decreases with \emph{max\_depth} $=\infty$, the decision tree recursion does reach a point with only one element in the subset of data to be trained on, and hence with no split variable returned it satisfies the base condition eventually to end recursion.
\item \items {
\item \blu{\href{https://github.ubc.ca/cpsc340/jaysc96_hw2/blob/master/code/random_stump.py}{random\_stump.py}}
\item \begin{center}
\fig{0.7}{../figs/2_2a3RandomTree} %
\end{center}}
\item \begin{center}
\fig{0.7}{../figs/2_2a4DecisionTree} %
\end{center}
}

\subsection{Random Decision Forests}
\rubric{code:4, reasoning:2}

The function \emph{decision\_forest.py} model repeatedly calls \emph{decision\_tree} to fit a set of models, and for prediction averages their results.
\blu{
\enum{
\item Report the test error of the \emph{decision\_forest} classifier with a depth of $\infty$ and $50$ trees.
\item Report the test error of the \emph{decision\_forest} classifier with a depth of $\infty$ and $50$ trees, if each tree is trained on a different boostrap sample.
\item Report the test error of the \emph{decision\_forest} classifier with a depth of $\infty$ and $50$ trees, if you train on the original dataset but use \emph{randomTree} instead \emph{decisionTree} to fit the models.
\item Report the test error of the \emph{decision\_forest} classifier with a depth of $\infty$ and $50$ trees, if you use \emph{random\_tree} and each tree is trained on a different bootstrap sample. Hand in your modified \emph{decision\_forest.py} file (which is now a random forest model).
\item What is the effect of the two ingredients of random forests, bootstrapping and random splits, on the performance on this dataset?
}
}

\enum {
\item Testing error for $decision\_forest=0.3674$
\item Testing error for $decision\_forest$ with bootstrapping $=0.2614$
\item Testing error for $random\_forest=0.1780$
\item Testing error for $random\_forest$ with bootstrapping $=0.1629$
\item Instead of going through all features, random splits fit models quicker acting on fewer features which are randomly chosen which greatly improves performance in case there are many features. Random forests use bootstrap samples to fit and average out predictions returned by multiple trees returning a more accurate answer than just one decision tree.
}

\section{K-Means Clustering}

If you run \verb|python main.py -q 3.1|, it will load a dataset with two features and a very obvious clustering structure. It will then apply the $k$-means algorithm with a random initialization. The result of applying the algorithm will thus depend on the randomization, but a typical run might look like this:
\centerfig{.5}{../figs/kmeans_rinit1.png}
But the `correct' clustering (that was used to make the data) is this:
\centerfig{.5}{../figs/kmeans_rinit2.png}
(Note that the colours are arbitrary.)

\subsection{Selecting among Initializations}
\rubric{code:4}

If you run the demo several times, it will find different clusterings. To select among clusterings for a \emph{fixed} value of $k$, one strategy is to minimize the sum of squared distances between examples $x_i$ and their means $w_{c_i}$,
\[
f(w_1,w_2,\dots,w_k,c_1,c_2,\dots,c_n) = \sum_{i=1}^n \norm{x_i - w_{c_i}}_2^2 = \sum_{i=1}^n \sum_{j=1}^d (x_{ij} - w_{c_ij})^2.
\]
 where $c_i$ is the \emph{index} of the closest mean to $x_i$ (an integer), and $w_{c_i}$ is the \emph{location} of the closest mean to $x_i$ (a vector in $\mathbb{R}^d$). This is a natural criterion because the steps of $k$-means alternately optimize this objective function in terms of the $w_c$ and the $c_i$ values.

 \blu{\enum{
 \item In the \emph{kmeans.py} file, add a new function called \emph{error} that takes the same input as the \emph{predict} function but that returns the value of this above objective function. Hand in your code.
 \item Using the \emph{utils.plot\_2dclustering} function, output the clustering obtained by running $k$-means 50 times (with $k=4$) and taking the one with the lowest error.
 }}

\enum {
\item \blu{\href{https://github.ubc.ca/cpsc340/jaysc96_hw2/blob/master/code/kmeans.py}{kmeans.py}}
\item \begin{center}
\fig{0.7}{../figs/2_3aKMeanCluster} %
\end{center}
}

 \subsection{Selecting $k$}
 \rubric{reasoning:4}

 We now turn to the much-more-difficult task of choosing the number of clusters $k$.

 \blu{\enum{
 \item Explain why the above objective function cannot be used to choose $k$.
 \item Explain why even evaluating this objective function on test data still wouldn't be a suitable approach to choosing $k$.
 \item Hand in a plot of the minimum error found across 50 random initializations, as you vary $k$ from $1$ to $10$.
 \item The \emph{elbow method} for choosing $k$ consists of looking at the above plot and visually trying to choose the $k$ that makes the sharpest ``elbow" (the biggest change in slope). What values of $k$ might be reasonable according to this method? Note: there is not a single correct answer here; it is somewhat open to interpretation and there is a range of reasonable answers.   
 }}

\enum{
\item The above objective function cannot be used to choose k, since it assumes the value of k is already known.
\item Evaluating the objective function on test data still wouldn’t be a suitable approach since we do not know about presence of outliers which will most likely give sub-optimal clusters.
\item \begin{center}
\fig{0.7}{../figs/2_3bChoosingK} %
\end{center}
\item Visually the sharpest elbows seem to be at $k=3,4$ which would be ideal for this clusterData.
}

\subsection{$k$-Medians}
\rubric{reasoning:3, code:3}

 The data in \emph{clusterData2} is the exact same as the above data, except it has 4 outliers that are very far away from the data.

 \blu{\enum{
 \item Using the \emph{plot\_2dclustering} function, output the clustering obtained by running $k$-means 50 times (with $k=4$) on \emph{clusterData2} and taking the one with the lowest error.
 \item What values of $k$ might be chosen by the elbow method for this dataset?
 \item Implement the $k$-\emph{medians} algorithm, which assigns examples to the nearest $w_c$ in the L1-norm and updates the $w_c$ by setting them to the ``median" of the points assigned to the cluster (we define the $d$-dimensional median as the concatenation of the median of the points along each dimension). Hand in your code.
\item Using the L1-norm version of the error (where $c_i$ now represents the closest median in the L1-norm),
\[
f(w_1,w_2,\dots,w_k,c_1,c_2,\dots,c_n) = \sum_{i=1}^n \norm{x_i - w_{c_i}}_1 = \sum_{i=1}^n \sum_{j=1}^d |x_{ij} - w_{c_ij}|,
\]
what value of $k$ would be chosen by the elbow method under this strategy?
 }}

\enum {
\item \begin{center}
\fig{0.7}{../figs/2_3cKMeanCluster2} %
\end{center}
\item Since means are sensitive to outliers, the elbow method would probably give us $k=7,8$ for this dataset as there are 4 outliers.
\item \items{
\item \blu{\href{https://github.ubc.ca/cpsc340/jaysc96_hw2/blob/master/code/kmedians.py}{kmedians.py}}
\item \begin{center}
\fig{0.7}{../figs/2_3cKMedianCluster2} %
\end{center}}
\item Since medians are not sensitive to outliers, the elbow method gives us $k=3,4$.
}

\section{Vector Quantization and Density-Based Clustering}

In this question we'll look at vector quantization, an alternative way that $k$-means is commonly used. Then we'll start to explore alternative clustering methods like density-based clustering.

\subsection{Image Colour-Space Compression}
\rubric{code:2}

Discovering object groups is one motivation for clustering. Another motivation is vector quantization, where we find a prototype point for each cluster and replace points in the cluster by their prototype.
The dataset \emph{dog} contains a 3D-array $I$ representing the RGB values of a picture of a dog. You can view the picture by using the \emph{plt.imshow} function. Create a file called \emph{quantize\_image.py} with the function \verb|def quantize_image(I,b)| where $I$ is an image, $b$ is the number of bits used to represent the colour-space, and returns a version of the image where each pixel's colour can be encoded using only $b$ bits by replacing the original pixel's colour with the nearest prototype. You should find the prototypes using the provided $k$-means code, and recall that with $b$ bits you can represent $2^b$ numbers, so the number of clusters should be $k = 2^b$. Hint: you may want to look up the \emph{np.reshape} command.
\blu{\enum{
\item Hand in your \emph{quantizeImage} function.
\item Show the image obtained if you encode the colours using $1$, $2$, $4$, and $6$ bits.
}}

\enum {
\item \blu{\href{https://github.ubc.ca/cpsc340/jaysc96_hw2/blob/master/code/quantize_image.py}{quantize\_image.py}}
\item \begin{center}
\newline $B=1$
\fig{0.7}{../figs/2_4aQuantizationB1} %
\newline $B=2$
\fig{0.7}{../figs/2_4aQuantizationB2} %
\newline $B=4$
\fig{0.7}{../figs/2_4aQuantizationB4} %
\newline $B=6$
\fig{0.7}{../figs/2_4aQuantizationB6} %
\end{center}
}

\subsection{Effect of Parameters on DBSCAN}
\rubric{code:2}

If you run \verb|python main -q 4.2|, it will apply the basic density-based clustering algorithm to the dataset from the previous question. The final output should look like this:
\centerfig{.7}{../figs/q4_2_dbscan.png}
Even though we know that each object was generated from one of four clusters (and we have 4 outliers), the algorithm finds 6 clusters and does not assign some objects to any cluster. However, the assignments will change if we change the parameters of the algorithm. Find and report values for the two parameters (\emph{radius2} and \emph{minPts}) such that the density-based clustering method finds:
\blu{\enum{
\item The 4 ``true" clusters.
\item 3 clusters (merging the top two, which also seems like a reasonable interpretation).
\item 2 clusters.
\item 1 cluster.
}}

Unimportant note: we formulated things in terms of \emph{radius2}, the radius squared, which is why we then take its square root in \emph{dbscan.py}. 

\enum{
\item $radius2=4$, $minPts=3$
\item $radius2=16$, $minPts=3$
\item $radius2=169$, $minPts=3$
\item $radius2=256$, $minPts=3$
}

\subsection{K-Means vs. DBSCAN Clustering}
\rubric{reasoning:2, code:2}

If you run \verb|python main -q 4.3|, it will load a dataset containing 85 attribute values for 50 animals. It will then apply a $k$-means clustering to the animals, and report the resulting clusters. The exact clustering will depend on the initialization of $k$-means and the value of $k$, but below is the result of one of the runs:
\items{
\item Cluster 1: killer+whale blue+whale hippopotamus humpback+whale seal walrus dolphin
\item Cluster 2: elephant ox sheep rhinoceros buffalo giant+panda pig cow
\item Cluster 3: skunk mole hamster squirrel rabbit mouse raccoon
\item Cluster 4: antelope horse moose spider+monkey gorilla chimpanzee giraffe zebra deer
\item Cluster 5: grizzly+bear beaver dalmatian persian+cat german+shepherd siamese+cat tiger leopard fox bat wolf chihuahua rat weasel otter bobcat lion polar+bear collie
}
Some of these groupings make sense (cluster 1 contains fairly-large animals that live in or near the water) while others do not (grizzly bears and bats are both in cluster 5).

\blu{Modify this demo to use the density-based clustering method, and report the clusters obtained if you set \emph{minPoints} to 3 and the radius such that it finds 5 clusters.}

$radius2=15$
\items{
\item Cluster 1: antelope horse moose ox sheep giraffe buffalo zebra deer pig cow
\item Cluster 2: dalmatian persian+cat german+shepherd siamese+cat mole tiger leopard fox hamster squirrel rabbit wolf chihuahua rat weasel bobcat mouse collie
\item Cluster 3: hippopotamus elephant rhinoceros
\item Cluster 4: blue+whale humpback+whale seal walrus dolphin
\item Cluster 5: spider+monkey gorilla chimpanzee
}

\end{document}
